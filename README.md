# ML Studio

Link to YouTube channel: [Machine Learning Studio](https://www.youtube.com/@machinelearningstudio)

## Deep Learning Series playlist

| <span style="background-color:#D5DBDB;"> Video</span> | <span style="background-color:#D5DBDB;">Description</span> |
| --- | --- |
| [Video 1: Top 10 Activaiton Functions](https://www.youtube.com/watch?v=56ZxEmGRt2k&t=27s) | Giving a review of activation functions |
| [Video 2: A Review of Top NN Optimizers](https://youtu.be/7m8f0hP8Fzo) | A review of top 16 optimization algorithms for training neural networks |


## Attention mechanism & Transformers playlist
| <span style="background-color:#D5DBDB;"> Video</span> | <span style="background-color:#D5DBDB;">Description</span> |
| --- | --- |
| [Video 1: Marix Multiplication Concept Explained](https://www.youtube.com/watch?v=VXG6WzS-Xb4) | Linear algebra concepts (pre-requisite to attention mechanism) |
| [Video 2: Self-Attention Using Scaled Dot-Product Approach](https://www.youtube.com/watch?v=1IKrHh2X0F0) | Understanding the self-attention mechanism, and intro to scaled dot-product attention |
| [Video 3: A Dive Into Multihead Attention, Self-Attention and Cross-Attention](https://www.youtube.com/watch?v=mmzRYGCfTzc) | Multihead Attention |
| [Video 4: Transformer Architecture](https://youtu.be/1h7T_-V5GI4) | Transformer |
| [Video 5: PostLN, PreLN and ResiDual Transformers](https://youtu.be/RsuSOylfN2I) | LayerNorm in Transformer |
| [Video 6: Variants of Multi-head Attention: MQA and GQA](https://youtu.be/pVP0bu8QA2w) | MQA and GQA |
| [Video 7: Efficient Self-Attention](https://youtu.be/LgsiwDRnXls) | Reducing Complexity |
| [Video 8: Implementing Linear-Complexity Attention](https://youtu.be/ulmex-d49cM) | Implementing Linear-Attention in PyTorch |

