# ML Studio

Link to YouTube channel: [Machine Learning Studio](https://www.youtube.com/@machinelearningstudio)

## Large Language Models (LLMs)
| <span style="background-color:#D5DBDB;"> Video</span> | <span style="background-color:#D5DBDB;">Description</span> |
| --- | --- |
| [Video 23: Enhancing LLMs (an overview)](https://youtu.be/jwjiordEqtM) | Enhancing LLMs with prompt engineering, RAG and fine-tuning  |
| [Video 24: Retrieval Augmented Generation (RAG)](https://youtu.be/MJVTf63OF5o) | RAG concept and implementation from scratch using PyMilvus  |
| [Video 25: Parameter Efficient Fine-Tuning (PEFT)](https://youtu.be/6J00ksLBW_g) | An overview of PEFT methods: Adapters, Prefix-tuning, LoRA, ...  |

## Deep Learning Series playlist

| <span style="background-color:#D5DBDB;"> Video</span> | <span style="background-color:#D5DBDB;">Description</span> |
| --- | --- |
| [Video 1: Top 10 Activaiton Functions](https://www.youtube.com/watch?v=56ZxEmGRt2k&t=27s) | Giving a review of activation functions |
| [Video 2: A Review of Top NN Optimizers](https://youtu.be/7m8f0hP8Fzo) | A review of top 16 optimization algorithms for training neural networks |


## Attention mechanism & Transformers playlist
| <span style="background-color:#D5DBDB;"> Video</span> | <span style="background-color:#D5DBDB;">Description</span> |
| --- | --- |
| [Video 1: Marix Multiplication Concept Explained](https://www.youtube.com/watch?v=VXG6WzS-Xb4) | Linear algebra concepts (pre-requisite to attention mechanism) |
| [Video 2: Self-Attention Using Scaled Dot-Product Approach](https://www.youtube.com/watch?v=1IKrHh2X0F0) | Understanding the self-attention mechanism, and intro to scaled dot-product attention |
| [Video 3: A Dive Into Multihead Attention, Self-Attention and Cross-Attention](https://www.youtube.com/watch?v=mmzRYGCfTzc) | Multihead Attention |
| [Video 4: Transformer Architecture](https://youtu.be/1h7T_-V5GI4) | Transformer |
| [Video 5: PostLN, PreLN and ResiDual Transformers](https://youtu.be/RsuSOylfN2I) | LayerNorm in Transformer |
| [Video 6: Variants of Multi-head Attention: MQA and GQA](https://youtu.be/pVP0bu8QA2w) | MQA and GQA |
| [Video 7: Efficient Self-Attention](https://youtu.be/LgsiwDRnXls) | Reducing Complexity |
| [Video 8: Implementing Linear-Complexity Attention](https://youtu.be/ulmex-d49cM) | Implementing Linear-Attention in PyTorch |
| [Video 9: Introducing a new seriws on Vision Transformers](https://www.youtube.com/watch?v=Qfp7IGv2LXo) | Introduction and outline of Vision Transformers series|
| [Video 10: Self Attention in Image Domain](https://www.youtube.com/watch?v=i3kYSAMMWT8) | Self-Attention in Image domain: Non-Local Module|
| [Video 11: Relative Self-Attention Explained](https://www.youtube.com/watch?v=XdlmDfa2hew) | Mechanics of Relative Self-Attention |
| [Video 12: Evolution of Self-Attention in Vision](https://www.youtube.com/watch?v=bavfa_Rr2f4&t=730s) | Attention-Augmented Conv. (AANet), Stand-Alone Self-Attention (SASA), and Stand-Alone Axial Attention (SAAA)|

