{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164fb917-0fdf-442d-96c8-da88fcf92edf",
   "metadata": {},
   "source": [
    "Implementing Linear-Complexity Attention in PyTorch\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351da9c-6f01-4cf9-bab7-54890a4c02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9a09c-4705-438b-907c-89a4373c5301",
   "metadata": {},
   "source": [
    "## Compute Query, Key and Value Matrices\n",
    "\n",
    "\n",
    "$\\text{Input: }\\ X\\in \\mathbb{R}^{N\\times d}$\n",
    "\n",
    "$\\text{Projection Matrices:}$  \n",
    " * $W_q\\in \\mathbb{R}^{d\\times d_q}$  \n",
    " * $W_k\\in \\mathbb{R}^{d\\times d_k}$  \n",
    " * $W_v\\in \\mathbb{R}^{d\\times d_v}$\n",
    " \n",
    "Fro simplicity, assume $d_q=d_k=d_v=d$\n",
    "\n",
    "$\\text{Q, K, V Projection:}$\n",
    "\n",
    "$$Q = XW_q$$\n",
    "$$K = XW_k$$\n",
    "$$V = XW_v$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4b02d-82b6-4ab4-bb83-deae4ac62f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "N = 1000\n",
    "d = 256  # aka d_model\n",
    "dk = dq = dv = d  # for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31a3ca-dbe6-4fc3-8876-6d62f5033d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random input X\n",
    "\n",
    "\n",
    "# build projection matrices Wq, Wk, Qv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b05988-c401-48b2-bbc0-858e773b4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Q, K, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffccb5-7303-478b-81a0-6c52b495a137",
   "metadata": {},
   "source": [
    "## Original Scaled Dot-product Attention $\\mathcal{O}(N^2)$\n",
    "\n",
    "Ref.: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1c3e6-5589-4b0d-93f5-e075a0af752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalization: applying softmax\n",
    "\n",
    "# Sanity-check: each row sums to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d939c-f1d0-4354-b0f2-f6fce1dbdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched tensors\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalization: applying softmax\n",
    "\n",
    "# Sanity-check: each row sums to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f6ab2-02ac-417c-a840-55d483c4e616",
   "metadata": {},
   "source": [
    "## Method 1: Efficient-Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Efficient Attention: Attention with Linear Complexities\"](https://arxiv.org/abs/1812.01243)\n",
    "\n",
    "$$\\hat{A}(Q, K, V)=\\sigma_\\text{row}(Q) \\left(\\sigma_\\text{col} (K)^\\top V\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd7ede-0a28-4cb9-8f62-a147856cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply sigma to Q and K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate (sigma_col K)^T x V\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate the final output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548fe8d-2cc1-4604-910c-c120c9c7b22d",
   "metadata": {},
   "source": [
    "## Method 2: Linear Attention using Kernels\n",
    "\n",
    "Ref.: [\"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\"](https://arxiv.org/abs/2006.16236)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\phi(Q)\\left(\\phi(K)^\\top V\\right)$$\n",
    "\n",
    "$\\text{where feature function }\\ \\phi(x) = ELU(x) + 1$\n",
    "\n",
    "<img src=\"elu.png\" alt=\"ELU(x)\" style=\"width:250px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dc410-36d7-474e-924d-9277522b353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = torch.nn.ELU()\n",
    "def phi(x):\n",
    "    return elu(x) + 1\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply phi to Q and K\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate (phi_K)^T x V\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "#  final output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e332c28-c47e-405d-96b6-98bf08764808",
   "metadata": {},
   "source": [
    "## Method 3: Linear Attention using Taylor Series Approximation $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\"](https://arxiv.org/abs/2007.14902)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\frac{V^\\top.\\mathbb{1}_N + Q^\\prime \\left({K^\\prime}^\\top V\\right)}{N+Q^\\prime \\left({K^\\prime}^\\top .\\mathbb{1}_N\\right)}$$\n",
    "\n",
    "$\\text{where }\\ $  \n",
    "* $Q^\\prime = \\frac{Q}{\\|Q\\|_2}$  \n",
    "* $K^\\prime = \\frac{K}{\\|K\\|_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03a658-1ffa-4a16-98ea-eda3b89183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector of ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f31582-7392-4544-ad3e-25dc5476b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalize Q and K -> Q_prime and K_prime\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate intermediate terms\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the final result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df67401-bf1e-4503-8b3d-35dde9347022",
   "metadata": {},
   "source": [
    "## Method 4: Linformer's Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linformer: Self-Attention with Linear Complexity\"](https://arxiv.org/abs/2006.04768)\n",
    "\n",
    "$\\text{Low-rank approximation: projection matrices }E,F\\in \\mathbb{R}^{r\\times N}$\n",
    "\n",
    "$\\longrightarrow EK \\in \\mathbb{R}^{r\\times d_k}$  \n",
    "$\\longrightarrow FV \\in \\mathbb{R}^{r\\times d_v}$\n",
    "\n",
    "$\\text{Context-mapping matrix P: }\\ \\ P=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)$\n",
    "\n",
    "\n",
    "$$\\hat{A}(Q,EK,EV)=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)FV$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f55459-b85e-4a79-b33b-61fe7cdb62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume r is 100\n",
    "r = 100\n",
    "\n",
    "# Define random projection matrices E and F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84624a-f6d1-4ba3-aec2-0dbb5c894ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Project K and V\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the context-mapping matrix P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the final output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
