{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164fb917-0fdf-442d-96c8-da88fcf92edf",
   "metadata": {},
   "source": [
    "Implementing Linear-Complexity Attention Mechanism in PyTorch\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a351da9c-6f01-4cf9-bab7-54890a4c02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9a09c-4705-438b-907c-89a4373c5301",
   "metadata": {},
   "source": [
    "## Compute Query, Key and Value Matrices\n",
    "\n",
    "\n",
    "$\\text{Input: }\\ X\\in \\mathbb{R}^{N\\times d}$\n",
    "\n",
    "$\\text{Projection Matrices:}$  \n",
    " * $W_q\\in \\mathbb{R}^{d\\times d_q}$  \n",
    " * $W_k\\in \\mathbb{R}^{d\\times d_k}$  \n",
    " * $W_v\\in \\mathbb{R}^{d\\times d_v}$\n",
    "\n",
    "$\\text{Q, K, V Projection:}$\n",
    "\n",
    "$$Q = XW_q$$\n",
    "$$K = XW_k$$\n",
    "$$V = XW_v$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e4b02d-82b6-4ab4-bb83-deae4ac62f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "N = 1000\n",
    "d = 256  # aka d_model\n",
    "dk = dq = dv = d  # for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31a3ca-dbe6-4fc3-8876-6d62f5033d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ffccb5-7303-478b-81a0-6c52b495a137",
   "metadata": {},
   "source": [
    "## Original Scaled Dot-product Attention $\\mathcal{O}(N^2)$\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1c3e6-5589-4b0d-93f5-e075a0af752e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "015f6ab2-02ac-417c-a840-55d483c4e616",
   "metadata": {},
   "source": [
    "## Method 1: Efficient-Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Efficient Attention: Attention with Linear Complexities\"](https://arxiv.org/abs/1812.01243)\n",
    "\n",
    "$$\\hat{A}(Q, K, V)=\\sigma_\\text{row}(Q) \\left(\\sigma_\\text{col} (K)^\\top V\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd7ede-0a28-4cb9-8f62-a147856cd192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4548fe8d-2cc1-4604-910c-c120c9c7b22d",
   "metadata": {},
   "source": [
    "## Method 2: Linear Attention using Kernels\n",
    "\n",
    "Ref.: [\"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\"](https://arxiv.org/abs/2006.16236)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\phi(Q)\\left(\\phi(K)^\\top V\\right)$$\n",
    "\n",
    "$\\text{where feature function }\\ \\phi(x) = ELU(x) + 1$\n",
    "\n",
    "\n",
    "<img src=\"elu.png\" alt=\"ELU(x)\" style=\"width:250px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dc410-36d7-474e-924d-9277522b353c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e332c28-c47e-405d-96b6-98bf08764808",
   "metadata": {},
   "source": [
    "## Method 3: Linear Attention using Taylor Series Approximation $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\"](https://arxiv.org/abs/2007.14902)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\frac{V^\\top.\\mathbb{1}_N + Q^\\prime \\left({K^\\prime}^\\top V\\right)}{N+Q^\\prime \\left({K^\\prime}^\\top .\\mathcal{1}_N\\right)}$$\n",
    "\n",
    "$\\text{where }\\ $  \n",
    "* $Q^\\prime = \\frac{Q}{\\|Q\\|_2}$  \n",
    "* $K^\\prime = \\frac{Q}{\\|K\\|_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43168a07-9067-4449-9665-bb159936fd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df67401-bf1e-4503-8b3d-35dde9347022",
   "metadata": {},
   "source": [
    "## Method 4: Linformer's Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linformer: Self-Attention with Linear Complexity\"](https://arxiv.org/abs/2006.04768)\n",
    "\n",
    "$\\text{Low-rank approximation: projection matrices }E,F\\in \\mathbb{R}^{r\\times N}$\n",
    "\n",
    "$\\longrightarrow EK \\in \\mathbb{R}^{r\\times d_k}$  \n",
    "$\\longrightarrow FV \\in \\mathbb{R}^{r\\times d_v}$\n",
    "\n",
    "$\\text{Context-mapping matrix P: }\\ \\ P=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)$\n",
    "\n",
    "\n",
    "$$\\hat{A}(Q,EK,EV)=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)EV$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f55459-b85e-4a79-b33b-61fe7cdb62e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe246f-2931-4817-adbf-43eacf729aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f0126-5641-42dc-8df6-a6ea2340b458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
