{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164fb917-0fdf-442d-96c8-da88fcf92edf",
   "metadata": {},
   "source": [
    "Implementing Linear-Complexity Attention in PyTorch\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a351da9c-6f01-4cf9-bab7-54890a4c02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9a09c-4705-438b-907c-89a4373c5301",
   "metadata": {},
   "source": [
    "## Compute Query, Key and Value Matrices\n",
    "\n",
    "\n",
    "$\\text{Input: }\\ X\\in \\mathbb{R}^{N\\times d}$\n",
    "\n",
    "$\\text{Projection Matrices:}$  \n",
    " * $W_q\\in \\mathbb{R}^{d\\times d_q}$  \n",
    " * $W_k\\in \\mathbb{R}^{d\\times d_k}$  \n",
    " * $W_v\\in \\mathbb{R}^{d\\times d_v}$\n",
    " \n",
    "Fro simplicity, assume $d_q=d_k=d_v=d$\n",
    "\n",
    "$\\text{Q, K, V Projection:}$\n",
    "\n",
    "$$Q = XW_q$$\n",
    "$$K = XW_k$$\n",
    "$$V = XW_v$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e4b02d-82b6-4ab4-bb83-deae4ac62f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "N = 1000\n",
    "d = 256  # aka d_model\n",
    "dk = dq = dv = d  # for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed31a3ca-dbe6-4fc3-8876-6d62f5033d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: torch.Size([1000, 256])\n",
      "Wq: torch.Size([256, 256])\n",
      "Wk: torch.Size([256, 256])\n",
      "Wv: torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Make a random input X\n",
    "X = torch.rand(N, d)\n",
    "print('Input X:', X.shape)\n",
    "\n",
    "\n",
    "# build projection matrices:\n",
    "Wq = torch.rand(d, dq)\n",
    "Wk = torch.rand(d, dk)\n",
    "Wv = torch.rand(d, dv)\n",
    "print('Wq:', Wq.shape)\n",
    "print('Wk:', Wk.shape)\n",
    "print('Wv:', Wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b05988-c401-48b2-bbc0-858e773b4726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: torch.Size([1000, 256])\n",
      "K: torch.Size([1000, 256])\n",
      "V: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "Q = X.matmul(Wq)\n",
    "K = X.matmul(Wk)\n",
    "V = X.matmul(Wv)\n",
    "print('Q:', Q.shape)\n",
    "print('K:', K.shape)\n",
    "print('V:', V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffccb5-7303-478b-81a0-6c52b495a137",
   "metadata": {},
   "source": [
    "## Original Scaled Dot-product Attention $\\mathcal{O}(N^2)$\n",
    "\n",
    "Ref.: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e1c3e6-5589-4b0d-93f5-e075a0af752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores: torch.Size([1000, 1000])\n",
      "attention_weights: torch.Size([1000, 1000])\n",
      "sum_over_rows is all 1? True\n",
      "attention_output: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention scores\n",
    "attention_scores = Q.matmul(K.t())\n",
    "print('attention_scores:', attention_scores.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalization: applying softmax\n",
    "attention_weights = torch.softmax(attention_scores / math.sqrt(dk), dim=-1)\n",
    "print('attention_weights:', attention_weights.shape)\n",
    "\n",
    "# Sanity-check: each row sums to 1\n",
    "sum_over_rows = torch.sum(attention_weights, dim=-1)\n",
    "print('sum_over_rows is all 1?', torch.allclose(sum_over_rows, torch.ones_like(sum_over_rows)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention output\n",
    "attention_output = attention_weights.matmul(V)\n",
    "print('attention_output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300d939c-f1d0-4354-b0f2-f6fce1dbdbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_Q: torch.Size([1, 1000, 256])\n",
      "batch_K: torch.Size([1, 1000, 256])\n",
      "batch_V: torch.Size([1, 1000, 256])\n",
      "attention_scores: torch.Size([1, 1000, 1000])\n",
      "attention_weights: torch.Size([1, 1000, 1000])\n",
      "sum_over_rows is all 1? True\n",
      "attention_output: torch.Size([1, 1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# batched tensors\n",
    "batch_Q = Q.unsqueeze(0)\n",
    "batch_K = K.unsqueeze(0)\n",
    "batch_V = V.unsqueeze(0)\n",
    "print('batch_Q:', batch_Q.shape)\n",
    "print('batch_K:', batch_K.shape)\n",
    "print('batch_V:', batch_V.shape)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the attention scores\n",
    "attention_scores = torch.bmm(batch_Q, batch_K.transpose(1, 2))\n",
    "print('attention_scores:', attention_scores.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalization: applying softmax\n",
    "attention_weights = torch.softmax(attention_scores / math.sqrt(dk), dim=-1)\n",
    "print('attention_weights:', attention_weights.shape)\n",
    "\n",
    "# Sanity-check: each row sums to 1\n",
    "sum_over_rows = torch.sum(attention_weights, dim=-1)\n",
    "print('sum_over_rows is all 1?', torch.allclose(sum_over_rows, torch.ones_like(sum_over_rows)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "attention_output = torch.bmm(attention_weights, batch_V)\n",
    "print('attention_output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f6ab2-02ac-417c-a840-55d483c4e616",
   "metadata": {},
   "source": [
    "## Method 1: Efficient-Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Efficient Attention: Attention with Linear Complexities\"](https://arxiv.org/abs/1812.01243)\n",
    "\n",
    "$$\\hat{A}(Q, K, V)=\\sigma_\\text{row}(Q) \\left(\\sigma_\\text{col} (K)^\\top V\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04cd7ede-0a28-4cb9-8f62-a147856cd192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_Q: torch.Size([1000, 256])\n",
      "sigma_K: torch.Size([1000, 256])\n",
      "step2_result: torch.Size([256, 256])\n",
      "attention_output: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply sigma to Q and K\n",
    "sigma_Q = torch.softmax(Q, dim=1)\n",
    "sigma_K = torch.softmax(K, dim=0)\n",
    "print('sigma_Q:', sigma_Q.shape)\n",
    "print('sigma_K:', sigma_K.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate (sigma_col K)^T x V\n",
    "step2_result = sigma_K.t().matmul(V)\n",
    "print('step2_result:', step2_result.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate the final output\n",
    "attention_output = sigma_Q.matmul(step2_result)\n",
    "print('attention_output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548fe8d-2cc1-4604-910c-c120c9c7b22d",
   "metadata": {},
   "source": [
    "## Method 2: Linear Attention using Kernels\n",
    "\n",
    "Ref.: [\"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\"](https://arxiv.org/abs/2006.16236)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\phi(Q)\\left(\\phi(K)^\\top V\\right)$$\n",
    "\n",
    "$\\text{where feature function }\\ \\phi(x) = ELU(x) + 1$\n",
    "\n",
    "<img src=\"elu.png\" alt=\"ELU(x)\" style=\"width:250px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6dc410-36d7-474e-924d-9277522b353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step2_result: torch.Size([256, 256])\n",
      "attention_output: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "elu = torch.nn.ELU()\n",
    "def phi(x):\n",
    "    return elu(x) + 1\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply phi to Q and K\n",
    "phi_Q = phi(Q)\n",
    "phi_K = phi(K)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate (phi_K)^T x V\n",
    "step2_result = phi_K.t().matmul(V)\n",
    "print('step2_result:', step2_result.shape)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "#  final output\n",
    "attention_output = phi_Q.matmul(step2_result)\n",
    "print('attention_output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e332c28-c47e-405d-96b6-98bf08764808",
   "metadata": {},
   "source": [
    "## Method 3: Linear Attention using Taylor Series Approximation $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\"](https://arxiv.org/abs/2007.14902)\n",
    "\n",
    "$$\\hat{A}(Q,K,V)=\\frac{V^\\top.\\mathbb{1}_N + Q^\\prime \\left({K^\\prime}^\\top V\\right)}{N+Q^\\prime \\left({K^\\prime}^\\top .\\mathbb{1}_N\\right)}$$\n",
    "\n",
    "$\\text{where }\\ $  \n",
    "* $Q^\\prime = \\frac{Q}{\\|Q\\|_2}$  \n",
    "* $K^\\prime = \\frac{K}{\\|K\\|_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a03a658-1ffa-4a16-98ea-eda3b89183c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_N: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Create a vector of ones\n",
    "one_N = torch.ones(N)\n",
    "print('one_N:', one_N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f31582-7392-4544-ad3e-25dc5476b919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_of_Q: torch.Size([1000, 1])\n",
      "norm_of_K: torch.Size([1000, 1])\n",
      "term1: torch.Size([256])\n",
      "term2: torch.Size([1000, 256])\n",
      "term3: torch.Size([1000])\n",
      "attention_output: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalize Q and K\n",
    "norm_of_Q = torch.norm(Q, p=2, dim=1, keepdim=True)\n",
    "norm_of_K = torch.norm(K, p=2, dim=1, keepdim=True)\n",
    "print('norm_of_Q:', norm_of_Q.shape)\n",
    "print('norm_of_K:', norm_of_K.shape)\n",
    "Q_prime = Q / norm_of_Q\n",
    "K_prime = K / norm_of_K\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate intermediate terms\n",
    "term1 = V.t().matmul(one_N)\n",
    "term2 = Q_prime.matmul(K_prime.t().matmul(V))\n",
    "term3 = Q_prime.matmul(K_prime.t().matmul(one_N))\n",
    "print('term1:', term1.shape)\n",
    "print('term2:', term2.shape)\n",
    "print('term3:', term3.shape)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the final result\n",
    "attention_output = (term1 + term2) / (N + term3.unsqueeze(-1))\n",
    "print('attention_output:', attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df67401-bf1e-4503-8b3d-35dde9347022",
   "metadata": {},
   "source": [
    "## Method 4: Linformer's Attention $\\mathcal{O}(N)$\n",
    "\n",
    "Ref.: [\"Linformer: Self-Attention with Linear Complexity\"](https://arxiv.org/abs/2006.04768)\n",
    "\n",
    "$\\text{Low-rank approximation: projection matrices }E,F\\in \\mathbb{R}^{r\\times N}$\n",
    "\n",
    "$\\longrightarrow EK \\in \\mathbb{R}^{r\\times d_k}$  \n",
    "$\\longrightarrow FV \\in \\mathbb{R}^{r\\times d_v}$\n",
    "\n",
    "$\\text{Context-mapping matrix P: }\\ \\ P=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)$\n",
    "\n",
    "\n",
    "$$\\hat{A}(Q,EK,EV)=\\text{Softmax}\\left(\\frac{Q(EK)^\\top}{\\sqrt{d_k}}\\right)FV$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f55459-b85e-4a79-b33b-61fe7cdb62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume r is 100\n",
    "r = 100\n",
    "\n",
    "# Define random projection matrices E and F\n",
    "E = torch.rand(r, N)\n",
    "F = torch.rand(r, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd84624a-f6d1-4ba3-aec2-0dbb5c894ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EK: torch.Size([100, 256])\n",
      "FV: torch.Size([100, 256])\n",
      "Context-mapping matrix P: torch.Size([1000, 100])\n",
      "attention_output: torch.Size([1000, 256])\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~~~~~~~~ Step 1 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Project K and V\n",
    "EK = E.matmul(K)\n",
    "FV = F.matmul(V)\n",
    "print('EK:', EK.shape)\n",
    "print('FV:', FV.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 2 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the context-mapping matrix P\n",
    "P = torch.softmax(Q.matmul(EK.t()) / math.sqrt(dk), dim=1)\n",
    "print('Context-mapping matrix P:', P.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~ Step 3 ~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute the final output\n",
    "attention_output = P.matmul(FV)\n",
    "print('attention_output:', attention_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
