{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade openai pymilvus\n",
    "!{sys.executable} -m pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78808 11755\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf_filepath = \"/Users/mlstudio/Documents/papers/RAG/VisRAG.pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "text = extract_text_from_pdf(pdf_filepath)\n",
    "# clean text\n",
    "text = text.replace(\".\\n\", \".NEWLINE\")\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "text = text.replace(\".NEWLINE\", \".\\n\")\n",
    "print(len(text), len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 280\n"
     ]
    }
   ],
   "source": [
    "def split_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start: end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text, 2000, 500)\n",
    "print(len(chunks), len(chunks[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlstudio/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "# Generate embeddings\n",
    "embeddings = [model.encode([chunk]) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "milvus_client = MilvusClient(uri=\"milvus_openai_RAG.db\")\n",
    "\n",
    "# Create a collection\n",
    "COLLECTION_NAME = \"visRAG_paper\"\n",
    "DIMENSION = 384\n",
    "if milvus_client.has_collection(collection_name=COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME, dimension=DIMENSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data\n",
    "data = [\n",
    "    {\n",
    "        \"id\": i, \"vector\": embeddings[i][0].tolist(),\n",
    "        \"text\": chunks[i], \"subject\": \"VisRAG\"\n",
    "    }\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "res = milvus_client.insert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=data\n",
    ")\n",
    "res[\"insert_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Query: In VisRAG-retrieval, how the final embedding is generated?\n",
      "\n",
      "0: chunk_id=10 dist=0.625\n",
      "en q. We follow the dual-encoder paradigm in text-\n",
      "\n",
      "1: chunk_id=24 dist=0.581\n",
      "Across the six evaluation datasets, VisRAG shows a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"In VisRAG-retrieval, how the final embedding is generated?\"\n",
    "\n",
    "query_vector = model.encode([query])[0].tolist()\n",
    "\n",
    "retrieved = milvus_client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=[query_vector],\n",
    "    limit=2,\n",
    "    output_fields=[\"text\"]\n",
    ")\n",
    "print(len(retrieved))\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for j, ret in enumerate(retrieved[0]):\n",
    "    print(f\"\\n{j}: chunk_id={ret['id']} dist={ret['distance']:.3f}\")\n",
    "    print(ret['entity']['text'][:50])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In VisRAG-retrieval, the final embedding is generated by combining visual and text features to enhance retrieval performance. The approach involves processing images and associated text using specialized models. Typically, a visual backbone model extracts features from images, while a text encoder processes associated textual data. These features are then combined or fused to produce a joint embedding that captures information from both modalities. This multimodal embedding is used to improve retrieval performance by leveraging the synergy between visual and textual information. The specific details and techniques used for feature extraction, fusion, and embedding generation might vary, and for exact details, reviewing the VisRAG paper would provide precise methodologies applied.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Answer the question about the VisRAG paper:\\n\"\n",
    "\n",
    "# get natural language response (without retrieved context)\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    store=True,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": PROMPT + query},\n",
    "    ]\n",
    ").to_dict()\n",
    "\n",
    "print(completion[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In VisRAG-retrieval, the final embedding is generated by employing a position-weighted mean pooling over the last-layer hidden states of a VLM (Vision-Language Model). This process involves separately encoding the query and page as text and image in the VLM, resulting in a sequence of hidden states. The position-weighted mean pooling gives higher weights to the later tokens in the sequence. The formula for deriving the final embedding \\( v \\) is:\n",
      "\n",
      "\\[ \n",
      "v = \\sum_{i=1}^{S} w_i h_i \n",
      "\\]\n",
      "\n",
      "where \\( h_i \\) is the i-th hidden state, \\( S \\) is the sequence length, and \\( w_i \\) is the i-th weight calculated as \n",
      "\n",
      "\\[ \n",
      "w_i = \\frac{i}{\\sum_{j=1}^{S} j} \n",
      "\\]\n",
      "\n",
      "The position-weighted pooling approach leverages the causal attention mechanism of generative VLMs, emphasizing the importance of later tokens in forming the final embedding.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = (\n",
    "    \"Answer the question about the VisRAG paper \"\n",
    "    \"based on the following context:\\n\"\n",
    ")\n",
    "context = \"\\n\".join([r[\"entity\"][\"text\"] for r in retrieved[0]])\n",
    "prompt = PROMPT + context + \"\\nQUESTION:\" + query\n",
    "\n",
    "# get natural language response\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    store=True,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ").to_dict()\n",
    "\n",
    "print(completion[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
